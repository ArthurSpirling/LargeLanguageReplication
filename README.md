# Replication for Language Models: Problems, Principles, and Best Practice for Political Science

Paper and related materials for Barrie, Palmer and Spirling (2024). The abstract for the paper is as follows:  

> Excitement about Language Models (LMs) abounds: these generative tools require minimal researcher input and yet make it possible to annotate and generate large quantities of data. But while LMs promise to replace conventional approaches to our efforts, there has been almost no systematic research into the reproducibility of research using these methods. This is a problem: the status quo for their use lacks the scientific integrity we expect in our field. We give a new theoretical framework for replication in the discipline and show that much LM work is uniquely wanting. We then demonstrate the problem empirically using a rolling iterated replication design in which we compare crowdsourcing and LMs on repeated, multiple tasks, over a long period of time (six months). We find that while LMs can match or exceed the accuracy of crowdworkers, the observed variance in LM performance is unexpectedly and unacceptably high. Indeed, in many cases the LM findings cannot be re-run, let alone replicated. We conclude with preliminary recommendations for best practice.

You can find the working paper here and a non-technical explainer here.
